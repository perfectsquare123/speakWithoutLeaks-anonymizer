{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "79812179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "     ------------------------------------- 491.5/491.5 kB 10.2 MB/s eta 0:00:00\n",
      "Collecting pyffx\n",
      "  Using cached pyffx-0.3.0.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp311-cp311-win_amd64.whl (212.5 MB)\n",
      "     -------------------------------------- 212.5/212.5 MB 6.0 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp311-cp311-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 8.4 MB/s eta 0:00:00\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp311-cp311-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 7.2 MB/s eta 0:00:00\n",
      "Collecting accelerate>=0.26.0\n",
      "  Using cached accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Collecting tenseal\n",
      "  Using cached tenseal-0.3.16-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "Collecting opacus\n",
      "  Using cached opacus-1.5.3-py3-none-any.whl (251 kB)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.8.5-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "Collecting stanza\n",
      "  Using cached stanza-1.10.1-py3-none-any.whl (1.1 MB)\n",
      "Collecting flair\n",
      "  Using cached flair-0.15.1-py3-none-any.whl (1.2 MB)\n",
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Collecting sdv\n",
      "  Downloading sdv-1.20.1-py3-none-any.whl (158 kB)\n",
      "     -------------------------------------- 158.3/158.3 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.19.11-py3-none-win_amd64.whl (20.8 MB)\n",
      "     ---------------------------------------- 20.8/20.8 MB 4.0 MB/s eta 0:00:00\n",
      "Collecting cryptography\n",
      "  Downloading cryptography-44.0.3-cp39-abi3-win_amd64.whl (3.2 MB)\n",
      "     ---------------------------------------- 3.2/3.2 MB 7.1 MB/s eta 0:00:00\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Collecting faker\n",
      "  Using cached faker-37.1.0-py3-none-any.whl (1.9 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\cihui\\documents\\github\\speakwithoutleaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\lib\\site-packages (from -r requirements.txt (line 20)) (2.2.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\cihui\\documents\\github\\speakwithoutleaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\lib\\site-packages (from -r requirements.txt (line 21)) (2.2.3)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-20.0.0-cp311-cp311-win_amd64.whl (25.8 MB)\n",
      "     ---------------------------------------- 25.8/25.8 MB 4.7 MB/s eta 0:00:00\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Collecting requests>=2.32.2\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting tqdm>=4.66.3\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Collecting fsspec[http]<=2025.3.0,>=2023.1.0\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "     -------------------------------------- 193.6/193.6 kB 5.9 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.24.0\n",
      "  Downloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
      "     -------------------------------------- 484.3/484.3 kB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\cihui\\documents\\github\\speakwithoutleaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\lib\\site-packages (from datasets->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: six in c:\\users\\cihui\\documents\\github\\speakwithoutleaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\lib\\site-packages (from pyffx->-r requirements.txt (line 2)) (1.17.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\cihui\\documents\\github\\speakwithoutleaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (4.13.2)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 6.3/6.3 MB 7.6 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Collecting jinja2\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Using cached pillow-11.2.1-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "Requirement already satisfied: psutil in c:\\users\\cihui\\documents\\github\\speakwithoutleaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\lib\\site-packages (from accelerate>=0.26.0->-r requirements.txt (line 7)) (7.0.0)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Collecting scipy>=1.2\n",
      "  Using cached scipy-1.15.2-cp311-cp311-win_amd64.whl (41.2 MB)\n",
      "Collecting opt-einsum>=3.3.0\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.12-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4\n",
      "  Using cached thinc-8.3.6-cp311-cp311-win_amd64.whl (1.8 MB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Downloading typer-0.15.3-py3-none-any.whl (45 kB)\n",
      "     -------------------------------------- 45.3/45.3 kB 321.2 kB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "     -------------------------------------- 443.9/443.9 kB 9.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\cihui\\documents\\github\\speakwithoutleaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (65.5.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Collecting emoji\n",
      "  Using cached emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "Collecting protobuf>=3.15.0\n",
      "  Using cached protobuf-6.30.2-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Collecting boto3>=1.20.27\n",
      "  Downloading boto3-1.38.11-py3-none-any.whl (139 kB)\n",
      "     -------------------------------------- 139.9/139.9 kB 8.1 MB/s eta 0:00:00\n",
      "Collecting conllu<5.0.0,>=4.0\n",
      "  Using cached conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Collecting deprecated>=1.2.13\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Collecting ftfy>=6.1.0\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Collecting gdown>=4.4.0\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Collecting langdetect>=1.0.9\n",
      "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting lxml>=4.8.0\n",
      "  Downloading lxml-5.4.0-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "     ---------------------------------------- 3.8/3.8 MB 7.9 MB/s eta 0:00:00\n",
      "Collecting matplotlib>=2.2.3\n",
      "  Using cached matplotlib-3.10.1-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "Collecting more-itertools>=8.13.0\n",
      "  Downloading more_itertools-10.7.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.3/65.3 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting mpld3>=0.3\n",
      "  Using cached mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
      "Collecting pptree>=3.1\n",
      "  Using cached pptree-3.1.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cihui\\documents\\github\\speakwithoutleaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\lib\\site-packages (from flair->-r requirements.txt (line 12)) (2.9.0.post0)\n",
      "Collecting pytorch-revgrad>=0.2.0\n",
      "  Using cached pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Collecting scikit-learn>=1.0.2\n",
      "  Using cached scikit_learn-1.6.1-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "Collecting segtok>=1.5.11\n",
      "  Using cached segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting sqlitedict>=2.0.0\n",
      "  Using cached sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tabulate>=0.8.10\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.3\n",
      "  Using cached transformer_smaller_training_vocab-0.4.1-py3-none-any.whl (14 kB)\n",
      "Collecting wikipedia-api>=0.5.7\n",
      "  Using cached wikipedia_api-0.8.1.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting bioc<3.0.0,>=2.0.0\n",
      "  Using cached bioc-2.1-py3-none-any.whl (33 kB)\n",
      "Collecting botocore<2.0.0,>=1.31\n",
      "  Downloading botocore-1.38.11-py3-none-any.whl (13.5 MB)\n",
      "     ---------------------------------------- 13.5/13.5 MB 5.7 MB/s eta 0:00:00\n",
      "Collecting cloudpickle>=2.1.0\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting graphviz>=0.13.2\n",
      "  Using cached graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Collecting copulas>=0.12.1\n",
      "  Using cached copulas-0.12.2-py3-none-any.whl (52 kB)\n",
      "Collecting ctgan>=0.11.0\n",
      "  Using cached ctgan-0.11.0-py3-none-any.whl (24 kB)\n",
      "Collecting deepecho>=0.7.0\n",
      "  Using cached deepecho-0.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting rdt>=1.16.0\n",
      "  Using cached rdt-1.16.0-py3-none-any.whl (69 kB)\n",
      "Collecting sdmetrics>=0.20.1\n",
      "  Using cached sdmetrics-0.20.1-py3-none-any.whl (193 kB)\n",
      "Requirement already satisfied: platformdirs>=4.0 in c:\\users\\cihui\\documents\\github\\speakwithoutleaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\lib\\site-packages (from sdv->-r requirements.txt (line 14)) (4.3.8)\n",
      "Collecting click\n",
      "  Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "     -------------------------------------- 307.7/307.7 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Collecting sentry-sdk>=2.0.0\n",
      "  Downloading sentry_sdk-2.27.0-py2.py3-none-any.whl (340 kB)\n",
      "     -------------------------------------- 340.8/340.8 kB 7.0 MB/s eta 0:00:00\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.6-cp311-cp311-win_amd64.whl (12 kB)\n",
      "Collecting cffi>=1.12\n",
      "  Using cached cffi-1.17.1-cp311-cp311-win_amd64.whl (181 kB)\n",
      "Requirement already satisfied: tzdata in c:\\users\\cihui\\documents\\github\\speakwithoutleaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\lib\\site-packages (from faker->-r requirements.txt (line 19)) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cihui\\documents\\github\\speakwithoutleaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 21)) (2025.2)\n",
      "Collecting jsonlines>=1.2.0\n",
      "  Using cached jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Collecting intervaltree\n",
      "  Using cached intervaltree-3.1.0.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docopt\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.13.0,>=0.12.0\n",
      "  Downloading s3transfer-0.12.0-py3-none-any.whl (84 kB)\n",
      "     ---------------------------------------- 84.8/84.8 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting urllib3!=2.2.0,<3,>=1.25.4\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\cihui\\documents\\github\\speakwithoutleaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\lib\\site-packages (from click->nltk->-r requirements.txt (line 15)) (0.4.6)\n",
      "Collecting plotly>=5.10.0\n",
      "  Using cached plotly-6.0.1-py3-none-any.whl (14.8 MB)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.11.18-cp311-cp311-win_amd64.whl (443 kB)\n",
      "     -------------------------------------- 443.7/443.7 kB 5.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wcwidth in c:\\users\\cihui\\documents\\github\\speakwithoutleaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\lib\\site-packages (from ftfy>=6.1.0->flair->-r requirements.txt (line 12)) (0.2.13)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Collecting language-data>=1.2\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.2-cp311-cp311-win_amd64.whl (222 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.57.0-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Using cached kiwisolver-1.4.8-cp311-cp311-win_amd64.whl (71 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 6.2 MB/s eta 0:00:00\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl (105 kB)\n",
      "     -------------------------------------- 105.4/105.4 kB 6.3 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "     -------------------------------------- 159.6/159.6 kB 4.8 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting blis<1.4.0,>=1.3.0\n",
      "  Using cached blis-1.3.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4\n",
      "  Using cached thinc-8.3.4-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Collecting blis<1.3.0,>=1.2.0\n",
      "  Using cached blis-1.2.1-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting rich>=10.11.0\n",
      "  Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Using cached cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.6.0-cp311-cp311-win_amd64.whl (120 kB)\n",
      "     -------------------------------------- 120.9/120.9 kB 7.4 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.4.3-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Using cached propcache-0.3.1-cp311-cp311-win_amd64.whl (45 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Using cached yarl-1.20.0-cp311-cp311-win_amd64.whl (93 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Collecting marisa-trie>=1.1.0\n",
      "  Using cached marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "Collecting narwhals>=1.15.1\n",
      "  Downloading narwhals-1.38.1-py3-none-any.whl (338 kB)\n",
      "     -------------------------------------- 338.4/338.4 kB 7.0 MB/s eta 0:00:00\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\cihui\\documents\\github\\speakwithoutleaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 10)) (2.19.1)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Collecting sortedcontainers<3.0,>=2.0\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: tenseal, sqlitedict, sortedcontainers, sentencepiece, pptree, mpmath, docopt, cymem, xxhash, wrapt, wasabi, urllib3, typing-inspection, tqdm, threadpoolctl, tabulate, sympy, spacy-loggers, spacy-legacy, soupsieve, smmap, shellingham, setproctitle, safetensors, regex, pyyaml, PySocks, pyparsing, pyffx, pydantic-core, pycparser, pyarrow, protobuf, propcache, pillow, opt-einsum, numpy, networkx, narwhals, murmurhash, multidict, more-itertools, mdurl, MarkupSafe, marisa-trie, lxml, langdetect, kiwisolver, joblib, jmespath, intervaltree, idna, graphviz, ftfy, fsspec, frozenlist, fonttools, filelock, faker, emoji, docker-pycreds, dill, cycler, conllu, cloudpickle, cloudpathlib, click, charset-normalizer, certifi, catalogue, attrs, annotated-types, aiohappyeyeballs, yarl, srsly, smart-open, sentry-sdk, segtok, scipy, requests, pydantic, preshed, plotly, nltk, multiprocess, markdown-it-py, language-data, jsonlines, jinja2, gitdb, deprecated, contourpy, cffi, botocore, blis, beautifulsoup4, aiosignal, wikipedia-api, torch, scikit-learn, s3transfer, rich, matplotlib, langcodes, huggingface-hub, gitpython, cryptography, copulas, confection, bioc, aiohttp, wandb, typer, torchvision, torchaudio, tokenizers, thinc, stanza, sdmetrics, rdt, pytorch-revgrad, opacus, mpld3, gdown, deepecho, boto3, accelerate, weasel, transformers, datasets, ctgan, spacy, sentence_transformers, sdv, transformer-smaller-training-vocab, flair\n",
      "  Running setup.py install for sqlitedict: started\n",
      "  Running setup.py install for sqlitedict: finished with status 'done'\n",
      "  Running setup.py install for pptree: started\n",
      "  Running setup.py install for pptree: finished with status 'done'\n",
      "  Running setup.py install for docopt: started\n",
      "  Running setup.py install for docopt: finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: sqlitedict is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  DEPRECATION: pptree is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  DEPRECATION: docopt is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\Users\\\\cihui\\\\Documents\\\\GitHub\\\\speakWithoutLeaks-data-level-privacy-preserving-pipeline-for-llm\\\\.venv\\\\Lib\\\\site-packages\\\\regex\\\\_regex.cp311-win_amd64.pyd'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6b874c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sanitized_text</th>\n",
       "      <th>sanitized_pct</th>\n",
       "      <th>perturbed_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Doe's email is john.doe@example.com and h...</td>\n",
       "      <td>1</td>\n",
       "      <td>Patrick Thompson email is UqACt^NA!#en[2Qkuyy....</td>\n",
       "      <td>96.97</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice paid with credit card 4111-1111-1111-111...</td>\n",
       "      <td>0</td>\n",
       "      <td>Denise Haas paid with credit card &lt;REMOVED&gt; on...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michael called 123-456-7890 to book a flight t...</td>\n",
       "      <td>2</td>\n",
       "      <td>Casey Stark called 123-456-7890 to book a flig...</td>\n",
       "      <td>96.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contact jane.smith87@gmail.com or +1-234-567-8...</td>\n",
       "      <td>3</td>\n",
       "      <td>Contact &amp;)xFuQy01ZvRNx&gt;0RJQDDL or +1-234-567-8...</td>\n",
       "      <td>30.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben paid 23 dollar using his credit card 4111-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Anna Lopez paid 1000 dollar using his credit c...</td>\n",
       "      <td>99.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  John Doe's email is john.doe@example.com and h...      1   \n",
       "1  Alice paid with credit card 4111-1111-1111-111...      0   \n",
       "2  Michael called 123-456-7890 to book a flight t...      2   \n",
       "3  Contact jane.smith87@gmail.com or +1-234-567-8...      3   \n",
       "4  Ben paid 23 dollar using his credit card 4111-...      2   \n",
       "\n",
       "                                      sanitized_text  sanitized_pct  \\\n",
       "0  Patrick Thompson email is UqACt^NA!#en[2Qkuyy....          96.97   \n",
       "1  Denise Haas paid with credit card <REMOVED> on...         100.00   \n",
       "2  Casey Stark called 123-456-7890 to book a flig...          96.36   \n",
       "3  Contact &)xFuQy01ZvRNx>0RJQDDL or +1-234-567-8...          30.99   \n",
       "4  Anna Lopez paid 1000 dollar using his credit c...          99.07   \n",
       "\n",
       "   perturbed_label  \n",
       "0                3  \n",
       "1                0  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"output/sanitized_test_output.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c9a5520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ben paid 23 dollar using his credit card 4111-1111-1111-1111 on 5pm buying 3 packets of apples in New York.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ce18a23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dwayne Reynolds paid 1000 dollar using his credit card <REMOVED> on <REMOVED> buying 3 packets of apples in [GPE_001].'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sanitized_text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f8c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "94ac86c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\cihui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "nltk.download('words')\n",
    "english_words = set(words.words())\n",
    "\n",
    "def shannon_entropy(token):\n",
    "    from collections import Counter\n",
    "    prob = [freq / len(token) for freq in Counter(token).values()]\n",
    "    return -sum(p * math.log2(p) for p in prob)\n",
    "\n",
    "def is_suspicious(token):\n",
    "    entropy = shannon_entropy(token)\n",
    "    is_dict_word = token.lower() in english_words\n",
    "    contains_weird_chars = bool(re.search(r'[^a-zA-Z0-9]', token))\n",
    "    digit_ratio = sum(c.isdigit() for c in token) / (len(token) + 1e-5)\n",
    "    \n",
    "    # thresholds may require tuning\n",
    "    if entropy > 3.5 and (not is_dict_word or digit_ratio > 0.3 or contains_weird_chars):\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d601d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"In a panel discussion, Todd Lang, Chief Privacy Officer at Huawei, warned of the potential misuse of location data, citing the February 2,1965 data breach at Huawei, where over 500,000 user profiles‚Äîincluding email addresses like &)xFuQy01ZvRNx>0RJQDDL and partial social security numbers‚Äîwere leaked online.\"\n",
    "tokens = sentence.split()\n",
    "heuristic_flags = [is_suspicious(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "60ecef9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In a panel discussion, Todd Lang, Chief Privacy Officer at Huawei, warned of the potential misuse of location data, citing the February 2,1965 data breach at Huawei, where over 500,000 user HERE email addresses like HERE and partial social security numbers‚Äîwere leaked online.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(tokens)):\n",
    "    if heuristic_flags[i] == True:\n",
    "        \n",
    "        tokens[i] = \"HERE\"\n",
    "        \n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9228d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers termcolor torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ed1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cihui\\Documents\\GitHub\\speakWithoutLeaks-data-level-privacy-preserving-pipeline-for-llm\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from termcolor import colored\n",
    "import re\n",
    "from nltk.corpus import words\n",
    "english_vocab = set(words.words())\n",
    "\n",
    "MODEL_NAME = \"perfectsquare123/encrypted-token-detector\"\n",
    "CONFIDENCE_THRESHOLD = 0.85\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME)\n",
    "id2label = model.config.id2label\n",
    "\n",
    "def is_false_positive_candidate(token):\n",
    "    if len(token) <= 1:\n",
    "        return True\n",
    "    if re.match(r\"\\[?[A-Z]{2,}[0-9]{3,}\\]?\", token): # for tokenized [PERSON001]\n",
    "        return True\n",
    "    if token.lower() in english_vocab:\n",
    "        return True\n",
    "    if re.match(r\"^[A-Z][a-z]+$\", token):  # common names\n",
    "        return True\n",
    "    if re.match(r\"<REDACT>\", token): # for redaction\n",
    "        return True\n",
    "    if re.match(r\".+'s$|.+'s$|.+s'$\", token): # e.g., \"John's\", \"boss'\"\n",
    "        return True\n",
    "    if re.match(r\"^(?:\\+65[- ]?)?[89]\\d{7}$\", token) or re.match(r\"^(\\+1[- ]?)?(\\(?\\d{3}\\)?[- ]?)?\\d{3}[- ]?\\d{4}$\", token): # sg or US phone number\n",
    "        return True\n",
    "    if re.match(r\"^(\\d{4}[- ]?){3}\\d{4}$\", token): # credit card number\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def highlight_encrypted_tokens_with_confidence(sentence):\n",
    "    tokens = sentence.strip().split()\n",
    "    inputs = tokenizer(tokens, return_tensors=\"pt\", is_split_into_words=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    predictions = torch.argmax(probs, dim=-1).squeeze().tolist()\n",
    "\n",
    "    word_ids = inputs.word_ids()\n",
    "    prev_word_idx = None\n",
    "\n",
    "    print(\"\\nüîç Detected Tokens:\")\n",
    "    for i, token_id in enumerate(inputs[\"input_ids\"][0]):\n",
    "        word_idx = word_ids[i]\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            continue\n",
    "\n",
    "        token = tokens[word_idx]\n",
    "        label_id = predictions[i]\n",
    "        label = id2label[label_id]\n",
    "        confidence = probs[0, i, label_id].item()\n",
    "\n",
    "        if label == \"B-ENC\" and confidence >= CONFIDENCE_THRESHOLD and not is_false_positive_candidate(token):\n",
    "            print(colored(f\"{token}({confidence:.2f})\", \"red\", attrs=[\"bold\"]), end=\" \")\n",
    "        else:\n",
    "            print(token, end=\" \")\n",
    "\n",
    "        prev_word_idx = word_idx\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5903703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence to detect encrypted tokens with confidence (Ctrl+C to exit):\n",
      "\n",
      "üîç Detected Tokens:\n",
      "John Doe's pencil case sent to [PERSON001] with \u001b[1m\u001b[31msbdfib()(&&^%Sjbs99180(1.00)\u001b[0m with +6589200549 phone and i think DKJBS8898*(&( \u001b[1m\u001b[31m0089JS(1.00)\u001b[0m that <REDACT> \n"
     ]
    }
   ],
   "source": [
    "# Interactive loop\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Enter a sentence to detect encrypted tokens with confidence (Ctrl+C to exit):\")\n",
    "    while True:\n",
    "        try:\n",
    "            sentence = input(\"> \")\n",
    "            highlight_encrypted_tokens_with_confidence(sentence)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2604cc98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
